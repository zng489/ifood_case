[2025-04-21T03:04:39.062-0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T03:04:39.076-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Scraping.run_scraper manual__2025-04-21T06:01:47.322667+00:00 [queued]>
[2025-04-21T03:04:39.087-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Scraping.run_scraper manual__2025-04-21T06:01:47.322667+00:00 [queued]>
[2025-04-21T03:04:39.088-0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 4
[2025-04-21T03:04:39.112-0300] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): run_scraper> on 2025-04-21 06:01:47.322667+00:00
[2025-04-21T03:04:39.118-0300] {standard_task_runner.py:72} INFO - Started process 30292 to run task
[2025-04-21T03:04:39.122-0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Scraping', 'run_scraper', 'manual__2025-04-21T06:01:47.322667+00:00', '--job-id', '54', '--raw', '--subdir', 'DAGS_FOLDER/Scraping.py', '--cfg-path', '/tmp/tmpgg_s8s5u']
[2025-04-21T03:04:39.124-0300] {standard_task_runner.py:105} INFO - Job 54: Subtask run_scraper
[2025-04-21T03:04:39.172-0300] {task_command.py:467} INFO - Running <TaskInstance: Scraping.run_scraper manual__2025-04-21T06:01:47.322667+00:00 [running]> on host DESKTOP-1GIT60U.
[2025-04-21T03:04:39.282-0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Scraping' AIRFLOW_CTX_TASK_ID='run_scraper' AIRFLOW_CTX_EXECUTION_DATE='2025-04-21T06:01:47.322667+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-04-21T06:01:47.322667+00:00'
[2025-04-21T03:04:39.284-0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-04-21T03:04:47.008-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:47.009-0300] {get_data_scraping.py:40} INFO - Acessando a página da NYC TLC...
[2025-04-21T03:04:53.590-0300] {get_data_scraping.py:44} INFO - Esperando botão de expansão de 2023...
[2025-04-21T03:04:53.840-0300] {get_data_scraping.py:47} INFO - Expansão da seção de 2023 realizada com sucesso.
[2025-04-21T03:04:53.841-0300] {get_data_scraping.py:49} INFO - Buscando todos os links de arquivos .parquet de 2023...
[2025-04-21T03:04:53.893-0300] {get_data_scraping.py:54} INFO - 12 arquivos encontrados para 2023.
[2025-04-21T03:04:53.966-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-01.parquet
[2025-04-21T03:04:53.967-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_01/yellow_tripdata_2023-01.parquet
[2025-04-21T03:04:53.968-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_01/yellow_tripdata_2023-01.parquet
[2025-04-21T03:04:53.968-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:04:54.547-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:54.588-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-02.parquet
[2025-04-21T03:04:54.589-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_02/yellow_tripdata_2023-02.parquet
[2025-04-21T03:04:54.590-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_02/yellow_tripdata_2023-02.parquet
[2025-04-21T03:04:54.590-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:04:55.266-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:55.294-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-03.parquet
[2025-04-21T03:04:55.295-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_03/yellow_tripdata_2023-03.parquet
[2025-04-21T03:04:55.295-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_03/yellow_tripdata_2023-03.parquet
[2025-04-21T03:04:55.295-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:04:55.851-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:55.865-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-04.parquet
[2025-04-21T03:04:55.865-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_04/yellow_tripdata_2023-04.parquet
[2025-04-21T03:04:55.865-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_04/yellow_tripdata_2023-04.parquet
[2025-04-21T03:04:55.866-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:04:56.393-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:56.408-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-05.parquet
[2025-04-21T03:04:56.409-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_05/yellow_tripdata_2023-05.parquet
[2025-04-21T03:04:56.409-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_05/yellow_tripdata_2023-05.parquet
[2025-04-21T03:04:56.410-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:04:57.117-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:57.135-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-06.parquet (mês 6 > 5)
[2025-04-21T03:04:57.149-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-07.parquet (mês 7 > 5)
[2025-04-21T03:04:57.162-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-08.parquet (mês 8 > 5)
[2025-04-21T03:04:57.179-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-09.parquet (mês 9 > 5)
[2025-04-21T03:04:57.193-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-10.parquet (mês 10 > 5)
[2025-04-21T03:04:57.209-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-11.parquet (mês 11 > 5)
[2025-04-21T03:04:57.221-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-12.parquet (mês 12 > 5)
[2025-04-21T03:04:57.222-0300] {get_data_scraping.py:116} INFO - ✅ Processo finalizado: arquivos até 2023-05 processados.
[2025-04-21T03:04:58.005-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:58.170-0300] {get_data_scraping.py:125} INFO - 🧹 Navegador fechado.
[2025-04-21T03:04:58.877-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:04:58.878-0300] {python.py:240} INFO - Done. Returned value was: None
[2025-04-21T03:04:58.885-0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-04-21T03:04:58.885-0300] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=Scraping, task_id=run_scraper, run_id=manual__2025-04-21T06:01:47.322667+00:00, execution_date=20250421T060147, start_date=20250421T060439, end_date=20250421T060458
[2025-04-21T03:04:58.946-0300] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-21T03:04:58.962-0300] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-21T03:04:58.964-0300] {local_task_job_runner.py:245} INFO - ::endgroup::
