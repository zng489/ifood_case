[2025-04-21T03:06:31.413-0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T03:06:31.442-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T03:06:31.454-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T03:06:31.454-0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 4
[2025-04-21T03:06:31.485-0300] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): run_scraper> on 2025-04-20 00:00:00+00:00
[2025-04-21T03:06:31.490-0300] {standard_task_runner.py:72} INFO - Started process 30840 to run task
[2025-04-21T03:06:31.494-0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Scraping', 'run_scraper', 'scheduled__2025-04-20T00:00:00+00:00', '--job-id', '57', '--raw', '--subdir', 'DAGS_FOLDER/Scraping.py', '--cfg-path', '/tmp/tmp8ubpd5sc']
[2025-04-21T03:06:31.496-0300] {standard_task_runner.py:105} INFO - Job 57: Subtask run_scraper
[2025-04-21T03:06:31.553-0300] {task_command.py:467} INFO - Running <TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [running]> on host DESKTOP-1GIT60U.
[2025-04-21T03:06:31.656-0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Scraping' AIRFLOW_CTX_TASK_ID='run_scraper' AIRFLOW_CTX_EXECUTION_DATE='2025-04-20T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-20T00:00:00+00:00'
[2025-04-21T03:06:31.657-0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-04-21T03:06:37.181-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:37.182-0300] {get_data_scraping.py:40} INFO - Acessando a página da NYC TLC...
[2025-04-21T03:06:40.205-0300] {get_data_scraping.py:44} INFO - Esperando botão de expansão de 2023...
[2025-04-21T03:06:40.456-0300] {get_data_scraping.py:47} INFO - Expansão da seção de 2023 realizada com sucesso.
[2025-04-21T03:06:40.457-0300] {get_data_scraping.py:49} INFO - Buscando todos os links de arquivos .parquet de 2023...
[2025-04-21T03:06:40.507-0300] {get_data_scraping.py:54} INFO - 12 arquivos encontrados para 2023.
[2025-04-21T03:06:40.564-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-01.parquet
[2025-04-21T03:06:40.565-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_01/yellow_tripdata_2023-01.parquet
[2025-04-21T03:06:40.566-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_01/yellow_tripdata_2023-01.parquet
[2025-04-21T03:06:40.566-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:06:41.199-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:41.214-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-02.parquet
[2025-04-21T03:06:41.214-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_02/yellow_tripdata_2023-02.parquet
[2025-04-21T03:06:41.215-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_02/yellow_tripdata_2023-02.parquet
[2025-04-21T03:06:41.215-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:06:41.689-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:41.703-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-03.parquet
[2025-04-21T03:06:41.704-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_03/yellow_tripdata_2023-03.parquet
[2025-04-21T03:06:41.704-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_03/yellow_tripdata_2023-03.parquet
[2025-04-21T03:06:41.705-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:06:42.278-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:42.292-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-04.parquet
[2025-04-21T03:06:42.292-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_04/yellow_tripdata_2023-04.parquet
[2025-04-21T03:06:42.293-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_04/yellow_tripdata_2023-04.parquet
[2025-04-21T03:06:42.293-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:06:42.773-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:42.788-0300] {get_data_scraping.py:83} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-05.parquet
[2025-04-21T03:06:42.788-0300] {get_data_scraping.py:87} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_05/yellow_tripdata_2023-05.parquet
[2025-04-21T03:06:42.789-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_05/yellow_tripdata_2023-05.parquet
[2025-04-21T03:06:42.789-0300] {get_data_scraping.py:89} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T03:06:43.362-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:43.376-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-06.parquet (mês 6 > 5)
[2025-04-21T03:06:43.389-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-07.parquet (mês 7 > 5)
[2025-04-21T03:06:43.401-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-08.parquet (mês 8 > 5)
[2025-04-21T03:06:43.413-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-09.parquet (mês 9 > 5)
[2025-04-21T03:06:43.427-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-10.parquet (mês 10 > 5)
[2025-04-21T03:06:43.443-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-11.parquet (mês 11 > 5)
[2025-04-21T03:06:43.457-0300] {get_data_scraping.py:65} INFO - 🟡 Ignorando yellow_tripdata_2023-12.parquet (mês 12 > 5)
[2025-04-21T03:06:43.458-0300] {get_data_scraping.py:116} INFO - ✅ Processo finalizado: arquivos até 2023-05 processados.
[2025-04-21T03:06:44.154-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:44.278-0300] {get_data_scraping.py:125} INFO - 🧹 Navegador fechado.
[2025-04-21T03:06:44.818-0300] {logging_mixin.py:190} INFO - Mensagem enviada com sucesso.
[2025-04-21T03:06:44.820-0300] {python.py:240} INFO - Done. Returned value was: None
[2025-04-21T03:06:44.828-0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-04-21T03:06:44.828-0300] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=Scraping, task_id=run_scraper, run_id=scheduled__2025-04-20T00:00:00+00:00, execution_date=20250420T000000, start_date=20250421T060631, end_date=20250421T060644
[2025-04-21T03:06:44.921-0300] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-21T03:06:44.935-0300] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-21T03:06:44.936-0300] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-21T13:14:45.268-0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T13:14:45.277-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:14:45.281-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:14:45.282-0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 4
[2025-04-21T13:14:45.296-0300] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): run_scraper> on 2025-04-20 00:00:00+00:00
[2025-04-21T13:14:45.299-0300] {standard_task_runner.py:72} INFO - Started process 71378 to run task
[2025-04-21T13:14:45.301-0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Scraping', 'run_scraper', 'scheduled__2025-04-20T00:00:00+00:00', '--job-id', '82', '--raw', '--subdir', 'DAGS_FOLDER/Ifood_case.py', '--cfg-path', '/tmp/tmppj95xtyg']
[2025-04-21T13:14:45.302-0300] {standard_task_runner.py:105} INFO - Job 82: Subtask run_scraper
[2025-04-21T13:14:45.334-0300] {task_command.py:467} INFO - Running <TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [running]> on host DESKTOP-1GIT60U.
[2025-04-21T13:14:45.387-0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Scraping' AIRFLOW_CTX_TASK_ID='run_scraper' AIRFLOW_CTX_EXECUTION_DATE='2025-04-20T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-20T00:00:00+00:00'
[2025-04-21T13:14:45.387-0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-04-21T13:14:45.397-0300] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/jobs_functions/get_data_scraping.py", line 15, in run_scraper
    s3_uri = lendo()
             ^^^^^^^
  File "/home/lenovo/airflow/jobs/s3/s3_functions.py", line 100, in lendo
    with mock_aws("s3"):
         ^^^^^^^^^^^^^^
TypeError: 'function' object does not support the context manager protocol
[2025-04-21T13:14:45.404-0300] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=Scraping, task_id=run_scraper, run_id=scheduled__2025-04-20T00:00:00+00:00, execution_date=20250420T000000, start_date=20250421T161445, end_date=20250421T161445
[2025-04-21T13:14:45.419-0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-04-21T13:14:45.419-0300] {standard_task_runner.py:124} ERROR - Failed to execute job 82 for task run_scraper ('function' object does not support the context manager protocol; 71378)
Traceback (most recent call last):
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/jobs_functions/get_data_scraping.py", line 15, in run_scraper
    s3_uri = lendo()
             ^^^^^^^
  File "/home/lenovo/airflow/jobs/s3/s3_functions.py", line 100, in lendo
    with mock_aws("s3"):
         ^^^^^^^^^^^^^^
TypeError: 'function' object does not support the context manager protocol
[2025-04-21T13:14:45.434-0300] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-21T13:14:45.450-0300] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-21T13:14:45.451-0300] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-21T13:25:04.290-0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T13:25:04.300-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:25:04.305-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:25:04.305-0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 4
[2025-04-21T13:25:04.322-0300] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): run_scraper> on 2025-04-20 00:00:00+00:00
[2025-04-21T13:25:04.325-0300] {standard_task_runner.py:72} INFO - Started process 72042 to run task
[2025-04-21T13:25:04.328-0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Scraping', 'run_scraper', 'scheduled__2025-04-20T00:00:00+00:00', '--job-id', '86', '--raw', '--subdir', 'DAGS_FOLDER/Ifood_case.py', '--cfg-path', '/tmp/tmpxxk9gyyh']
[2025-04-21T13:25:04.329-0300] {standard_task_runner.py:105} INFO - Job 86: Subtask run_scraper
[2025-04-21T13:25:04.364-0300] {task_command.py:467} INFO - Running <TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [running]> on host DESKTOP-1GIT60U.
[2025-04-21T13:25:04.416-0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Scraping' AIRFLOW_CTX_TASK_ID='run_scraper' AIRFLOW_CTX_EXECUTION_DATE='2025-04-20T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-20T00:00:00+00:00'
[2025-04-21T13:25:04.417-0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-04-21T13:25:04.426-0300] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/jobs_functions/get_data_scraping.py", line 15, in run_scraper
    s3_uri = lendo(is_mock = True)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/s3/s3_functions.py", line 101, in lendo
    with mock_aws("s3"):
         ^^^^^^^^^^^^^^
TypeError: 'function' object does not support the context manager protocol
[2025-04-21T13:25:04.433-0300] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=Scraping, task_id=run_scraper, run_id=scheduled__2025-04-20T00:00:00+00:00, execution_date=20250420T000000, start_date=20250421T162504, end_date=20250421T162504
[2025-04-21T13:25:04.449-0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-04-21T13:25:04.449-0300] {standard_task_runner.py:124} ERROR - Failed to execute job 86 for task run_scraper ('function' object does not support the context manager protocol; 72042)
Traceback (most recent call last):
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/jobs_functions/get_data_scraping.py", line 15, in run_scraper
    s3_uri = lendo(is_mock = True)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/s3/s3_functions.py", line 101, in lendo
    with mock_aws("s3"):
         ^^^^^^^^^^^^^^
TypeError: 'function' object does not support the context manager protocol
[2025-04-21T13:25:04.462-0300] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-21T13:25:04.474-0300] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-21T13:25:04.474-0300] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-21T13:29:10.557-0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T13:29:10.567-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:29:10.571-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:29:10.571-0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 4
[2025-04-21T13:29:10.585-0300] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): run_scraper> on 2025-04-20 00:00:00+00:00
[2025-04-21T13:29:10.588-0300] {standard_task_runner.py:72} INFO - Started process 72353 to run task
[2025-04-21T13:29:10.590-0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Scraping', 'run_scraper', 'scheduled__2025-04-20T00:00:00+00:00', '--job-id', '91', '--raw', '--subdir', 'DAGS_FOLDER/Ifood_case.py', '--cfg-path', '/tmp/tmppvwaqy6e']
[2025-04-21T13:29:10.591-0300] {standard_task_runner.py:105} INFO - Job 91: Subtask run_scraper
[2025-04-21T13:29:10.623-0300] {task_command.py:467} INFO - Running <TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [running]> on host DESKTOP-1GIT60U.
[2025-04-21T13:29:10.670-0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Scraping' AIRFLOW_CTX_TASK_ID='run_scraper' AIRFLOW_CTX_EXECUTION_DATE='2025-04-20T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-20T00:00:00+00:00'
[2025-04-21T13:29:10.671-0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-04-21T13:29:10.679-0300] {taskinstance.py:3311} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/jobs_functions/get_data_scraping.py", line 15, in run_scraper
    s3_uri = lendo(is_mock = True)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/s3/s3_functions.py", line 102, in lendo
    mock.start()  # Ativa o mock fora do contexto 'with'
    ^^^^^^^^^^
AttributeError: 'function' object has no attribute 'start'
[2025-04-21T13:29:10.684-0300] {taskinstance.py:1225} INFO - Marking task as UP_FOR_RETRY. dag_id=Scraping, task_id=run_scraper, run_id=scheduled__2025-04-20T00:00:00+00:00, execution_date=20250420T000000, start_date=20250421T162910, end_date=20250421T162910
[2025-04-21T13:29:10.698-0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-04-21T13:29:10.699-0300] {standard_task_runner.py:124} ERROR - Failed to execute job 91 for task run_scraper ('function' object has no attribute 'start'; 72353)
Traceback (most recent call last):
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/task/task_runner/standard_task_runner.py", line 117, in _start_by_fork
    ret = args.func(args, dag=self.dag)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/cli_config.py", line 49, in command
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/cli.py", line 116, in wrapper
    return f(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 483, in task_run
    task_return_code = _run_task_by_selected_method(args, _dag, ti)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 256, in _run_task_by_selected_method
    return _run_raw_task(args, ti)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/cli/commands/task_command.py", line 341, in _run_raw_task
    return ti._run_raw_task(
           ^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/session.py", line 97, in wrapper
    return func(*args, session=session, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3005, in _run_raw_task
    return _run_raw_task(
           ^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 273, in _run_raw_task
    TaskInstance._execute_task_with_callbacks(
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3159, in _execute_task_with_callbacks
    result = self._execute_task(context, task_orig)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 3183, in _execute_task
    return _execute_task(self, context, task_orig)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 767, in _execute_task
    result = _execute_callable(context=context, **execute_callable_kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/taskinstance.py", line 733, in _execute_callable
    return ExecutionCallableRunner(
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/models/baseoperator.py", line 422, in wrapper
    return func(self, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 238, in execute
    return_value = self.execute_callable()
                   ^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/operators/python.py", line 256, in execute_callable
    return runner.run(*self.op_args, **self.op_kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/miniconda3/envs/worker/lib/python3.12/site-packages/airflow/utils/operator_helpers.py", line 252, in run
    return self.func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/jobs_functions/get_data_scraping.py", line 15, in run_scraper
    s3_uri = lendo(is_mock = True)
             ^^^^^^^^^^^^^^^^^^^^^
  File "/home/lenovo/airflow/jobs/s3/s3_functions.py", line 102, in lendo
    mock.start()  # Ativa o mock fora do contexto 'with'
    ^^^^^^^^^^
AttributeError: 'function' object has no attribute 'start'
[2025-04-21T13:29:10.723-0300] {local_task_job_runner.py:266} INFO - Task exited with return code 1
[2025-04-21T13:29:10.739-0300] {taskinstance.py:3895} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-04-21T13:29:10.740-0300] {local_task_job_runner.py:245} INFO - ::endgroup::
[2025-04-21T13:37:51.255-0300] {local_task_job_runner.py:123} INFO - ::group::Pre task execution logs
[2025-04-21T13:37:51.264-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:37:51.268-0300] {taskinstance.py:2613} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [queued]>
[2025-04-21T13:37:51.268-0300] {taskinstance.py:2866} INFO - Starting attempt 1 of 4
[2025-04-21T13:37:51.282-0300] {taskinstance.py:2889} INFO - Executing <Task(PythonOperator): run_scraper> on 2025-04-20 00:00:00+00:00
[2025-04-21T13:37:51.285-0300] {standard_task_runner.py:72} INFO - Started process 72920 to run task
[2025-04-21T13:37:51.287-0300] {standard_task_runner.py:104} INFO - Running: ['airflow', 'tasks', 'run', 'Scraping', 'run_scraper', 'scheduled__2025-04-20T00:00:00+00:00', '--job-id', '96', '--raw', '--subdir', 'DAGS_FOLDER/Ifood_case.py', '--cfg-path', '/tmp/tmp00gs3hyp']
[2025-04-21T13:37:51.289-0300] {standard_task_runner.py:105} INFO - Job 96: Subtask run_scraper
[2025-04-21T13:37:51.321-0300] {task_command.py:467} INFO - Running <TaskInstance: Scraping.run_scraper scheduled__2025-04-20T00:00:00+00:00 [running]> on host DESKTOP-1GIT60U.
[2025-04-21T13:37:51.377-0300] {taskinstance.py:3132} INFO - Exporting env vars: AIRFLOW_CTX_DAG_EMAIL='airflow@example.com' AIRFLOW_CTX_DAG_OWNER='airflow' AIRFLOW_CTX_DAG_ID='Scraping' AIRFLOW_CTX_TASK_ID='run_scraper' AIRFLOW_CTX_EXECUTION_DATE='2025-04-20T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-04-20T00:00:00+00:00'
[2025-04-21T13:37:51.378-0300] {taskinstance.py:731} INFO - ::endgroup::
[2025-04-21T13:37:51.465-0300] {credentials.py:1213} INFO - Found credentials in environment variables.
[2025-04-21T13:37:51.777-0300] {logging_mixin.py:190} INFO - Lendo de: s3a://yellow_taxi_files/yellow_taxi_files/
[2025-04-21T13:37:51.778-0300] {logging_mixin.py:190} INFO - Caminho do S3 para leitura: s3a://yellow_taxi_files/yellow_taxi_files/
[2025-04-21T13:37:51.778-0300] {get_data_scraping.py:19} INFO - Aplicação iniciada
[2025-04-21T13:37:58.052-0300] {get_data_scraping.py:38} INFO - Acessando a página da NYC TLC...
[2025-04-21T13:38:01.862-0300] {get_data_scraping.py:42} INFO - Esperando botão de expansão de 2023...
[2025-04-21T13:38:02.000-0300] {get_data_scraping.py:45} INFO - Expansão da seção de 2023 realizada com sucesso.
[2025-04-21T13:38:02.001-0300] {get_data_scraping.py:47} INFO - Buscando todos os links de arquivos .parquet de 2023...
[2025-04-21T13:38:02.035-0300] {get_data_scraping.py:52} INFO - 12 arquivos encontrados para 2023.
[2025-04-21T13:38:02.052-0300] {get_data_scraping.py:81} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-01.parquet
[2025-04-21T13:38:02.052-0300] {get_data_scraping.py:84} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_01/yellow_tripdata_2023-01.parquet
[2025-04-21T13:38:02.053-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_01/yellow_tripdata_2023-01.parquet
[2025-04-21T13:38:02.053-0300] {get_data_scraping.py:86} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T13:38:02.785-0300] {credentials.py:1213} INFO - Found credentials in environment variables.
[2025-04-21T13:38:02.979-0300] {logging_mixin.py:190} INFO - Salvando em: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:02.980-0300] {logging_mixin.py:190} INFO - Caminho do S3 para escrita: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:02.988-0300] {get_data_scraping.py:81} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-02.parquet
[2025-04-21T13:38:02.988-0300] {get_data_scraping.py:84} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_02/yellow_tripdata_2023-02.parquet
[2025-04-21T13:38:02.988-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_02/yellow_tripdata_2023-02.parquet
[2025-04-21T13:38:02.989-0300] {get_data_scraping.py:86} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T13:38:03.444-0300] {credentials.py:1213} INFO - Found credentials in environment variables.
[2025-04-21T13:38:03.510-0300] {logging_mixin.py:190} INFO - Salvando em: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:03.511-0300] {logging_mixin.py:190} INFO - Caminho do S3 para escrita: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:03.518-0300] {get_data_scraping.py:81} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-03.parquet
[2025-04-21T13:38:03.518-0300] {get_data_scraping.py:84} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_03/yellow_tripdata_2023-03.parquet
[2025-04-21T13:38:03.518-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_03/yellow_tripdata_2023-03.parquet
[2025-04-21T13:38:03.519-0300] {get_data_scraping.py:86} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T13:38:03.976-0300] {credentials.py:1213} INFO - Found credentials in environment variables.
[2025-04-21T13:38:04.037-0300] {logging_mixin.py:190} INFO - Salvando em: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:04.038-0300] {logging_mixin.py:190} INFO - Caminho do S3 para escrita: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:04.044-0300] {get_data_scraping.py:81} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-04.parquet
[2025-04-21T13:38:04.045-0300] {get_data_scraping.py:84} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_04/yellow_tripdata_2023-04.parquet
[2025-04-21T13:38:04.045-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_04/yellow_tripdata_2023-04.parquet
[2025-04-21T13:38:04.045-0300] {get_data_scraping.py:86} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T13:38:04.482-0300] {credentials.py:1213} INFO - Found credentials in environment variables.
[2025-04-21T13:38:04.598-0300] {logging_mixin.py:190} INFO - Salvando em: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:04.598-0300] {logging_mixin.py:190} INFO - Caminho do S3 para escrita: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:04.606-0300] {get_data_scraping.py:81} WARNING - ⚠️ Arquivo já existe: yellow_tripdata_2023-05.parquet
[2025-04-21T13:38:04.606-0300] {get_data_scraping.py:84} INFO - 🚀 Simulando envio para S3: s3://yellow_taxi_files/2023_05/yellow_tripdata_2023-05.parquet
[2025-04-21T13:38:04.607-0300] {logging_mixin.py:190} INFO - 🚀 Enviando para S3: s3://yellow_taxi_files/2023_05/yellow_tripdata_2023-05.parquet
[2025-04-21T13:38:04.607-0300] {get_data_scraping.py:86} INFO - Simulando criação do path s3.upload_file(local_path, BUCKET_NAME, s3_key)
[2025-04-21T13:38:05.061-0300] {credentials.py:1213} INFO - Found credentials in environment variables.
[2025-04-21T13:38:05.143-0300] {logging_mixin.py:190} INFO - Salvando em: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:05.145-0300] {logging_mixin.py:190} INFO - Caminho do S3 para escrita: s3a://prd_yellow_taxi_table/yellow_taxi_output/
[2025-04-21T13:38:05.152-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-06.parquet (mês 6 > 5)
[2025-04-21T13:38:05.161-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-07.parquet (mês 7 > 5)
[2025-04-21T13:38:05.168-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-08.parquet (mês 8 > 5)
[2025-04-21T13:38:05.177-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-09.parquet (mês 9 > 5)
[2025-04-21T13:38:05.183-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-10.parquet (mês 10 > 5)
[2025-04-21T13:38:05.190-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-11.parquet (mês 11 > 5)
[2025-04-21T13:38:05.198-0300] {get_data_scraping.py:63} INFO - 🟡 Ignorando yellow_tripdata_2023-12.parquet (mês 12 > 5)
[2025-04-21T13:38:05.199-0300] {get_data_scraping.py:103} INFO - ✅ Processo finalizado: arquivos até 2023-05 processados.
[2025-04-21T13:38:05.766-0300] {get_data_scraping.py:112} INFO - 🧹 Navegador fechado.
[2025-04-21T13:38:06.187-0300] {python.py:240} INFO - Done. Returned value was: None
[2025-04-21T13:38:06.192-0300] {taskinstance.py:340} INFO - ::group::Post task execution logs
[2025-04-21T13:38:06.192-0300] {taskinstance.py:352} INFO - Marking task as SUCCESS. dag_id=Scraping, task_id=run_scraper, run_id=scheduled__2025-04-20T00:00:00+00:00, execution_date=20250420T000000, start_date=20250421T163751, end_date=20250421T163806
[2025-04-21T13:38:06.232-0300] {local_task_job_runner.py:266} INFO - Task exited with return code 0
[2025-04-21T13:38:06.245-0300] {taskinstance.py:3895} INFO - 1 downstream tasks scheduled from follow-on schedule check
[2025-04-21T13:38:06.255-0300] {local_task_job_runner.py:245} INFO - ::endgroup::
